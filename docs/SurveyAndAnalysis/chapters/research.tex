\chapter{Research}

Canonical Correlation Analysis (CCA) is a method for learning linear transformations of 2 random variables such that their representations are maximally correlated. In a multimodal setting, highly correlated representations would suggest that modalities are behaving the same way they did during training, whilst low correlation would suggest that one of the modalities is misbehaving. Clear limitations of this method, constraining it to linear transformations between 2 modalities, have been addressed separately by Deep CCA \cite{DCCA} which learns nonlinear transformations, and Generalised CCA [cite] which can be applied to arbitrarily many modalities. Deep Generalised CCA (DGCCA) \cite{DGCCA} addresses both simultaneously, being able to learn maximally correlated nonlinear representations between many views.

\cite{DGCCA} use these learned representations for K-nearest neighbour classification on XRMB and Twitter user datasets, obtaining better results than other CCA methods. Using a more complex classifier could give better results. Alternatively, DGCCA could be used to identify corrupt modalities for removal or reconstruction before running a separate model on the resulting data.

\cite{CRA} impute data from missing modalities using a Cascading Residual Autoencoder (CRA). A Residual Autoencoder (RA) is trained to minimise the difference between a complete training sample and the same sample with corruption. Multiple RAâ€™s are stacked, with the sum of the input and output of each RA, the current reconstruction, used as the input for the next RA. The difference between the current reconstruction and complete data reduces after each RA and deeper CRAs produce better imputations.
\cite{CRA} achieve good results with CRA on 4 datasets, outperforming all other matrix completion or autoencoder based imputation methods they test against. However, the datasets do not show much variation in modalities. For example in the HSFD dataset modalities are images of the same face taken in different spectral ranges, and in the RGB-D dataset the two modalities are RGB and Depth images of an object. It is unclear whether CRA could be effectively applied to modalities of vastly different types, such as audio, video, and caption data in movies.

\cite{DBM} Use a Deep Boltzmann Machine (DBM) to learn representations of multimodal data. A DBM is a multi-layered version of a Restricted Boltzmann Machine (RBM), a generative neural network that can learn a joint probability distribution over its inputs. They independently pretrain a pathway for each modality and fuse them into a single representation with a joint layer, resulting in a network which performed better than other unimodal and multimodal models at the time. When modalities are missing the model is given the remaining modalities and the missing data can be generated by sampling the missing modality pathways. This generated data can be fed back into the network along with the available modalities for inference, giving higher accuracy than using the available modalities by themselves.

Unlike previous methods mentioned, the MIR Flickr dataset used consists of two very different modalities, images and image tags. It was also adapted to use video and audio data from the CUAVE and AVLetters speech recognition datasets, achieving good results. This suggests a DBM could be a more generalizable method of multimodal learning than others and could be applied to more diverse modalities. Notably, the image features used were generated using PHOW, a shallow feature extractor from 2007, and modern image feature extraction methods such as CNNs could improve this performance. 

\cite{GANFootprint} use a Generative Adversarial Network (GAN) to reconstruct depth data from an RGB image. A GAN consists of a Generator network and a Discriminator network. Given an input, in this case RGB satellite data, the generator attempts to create a depth map. Rather than training this network with a loss function directly, the discriminator attempts to decide whether the generated depth map is real or fake and this decision is used to train the generator. Training is scheduled so that both networks get better at the same pace, resulting in a generator that car create realistic depth images from an RGB image. 
This method does give modest gains to accuracy over using the corrupted modality or ignoring the modality completely. However, as with CRA the two modalities are similar, both representing spacial data with features in similar positions. Again it is unclear whether these could be applied generally to multiple modalities.

\cite{ModDrop} introduce ModDrop, a modality-scale regularisation method which makes predictions robust to missing or corrupted modalities. ModDrop works similarly to the node-scale regularisation method of dropout \cite{dropout}, where each node in a layer is temporarily removed from the network with a specified probability. Dropout ensures the network does not overfit the training data and reduces reliance of the network on individual connections. ModDrop removes entire modalities instead, having a similar effect of reducing reliance on individual modalities. 
They compare networks trained with and without ModDrop on 2 datasets, MNIST with each corner of an image considered a separate modality, and the ChaLearn LAP gesture recognition dataset augmented with their own audio data. Their augmented ChaLearn dataset contains RGB-D streams for each hand, an audio stream, and a motion captured articulated pose, giving 6 modalities of 3 distinct types. Removing individual modalities caused a mean accuracy reduction of 9\%, compared to 23\% when ModDrop was not used, suggesting it is a good method of dealing with missing modalities. They also obtained good results when individual modalities were corrupted with 50\% pepper noise, though this does not necessarily model the types of corruption encountered in al life.
